<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Zhutian (Skye) Yang</title>
    <link rel="icon" href="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/favicon.ico?raw=true">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Zhutian (Skye) Yang is a PhD student in robotics at MIT">
    <link rel="stylesheet" href="css/main.css">
    <script>
      var blue = "#3498db";
      var darkblue = "#2980b9";
      function underlineSpan(x) {
        x.style.textDecoration = 'underline';
        x.style.color = $darkblue
      }

      function normalSpan(x) {
        x.style.textDecoration = 'none';
        x.style.color = $blue
      }

      
      var bibtex_piginet = `
  @INPROCEEDINGS{yang2023piginet, 
    AUTHOR    = {Zhutian  Yang AND Caelan R Garrett AND Lozano-P{\\'e}rez, Tom{\\'a}s AND Leslie Kaelbling AND Dieter Fox}, 
    TITLE     = {{Sequence-Based Plan Feasibility Prediction for Efficient Task and Motion Planning}}, 
    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
    YEAR      = {2023}, 
    ADDRESS   = {Daegu, Republic of Korea}, 
    MONTH     = {July}, 
    DOI       = {10.15607/RSS.2023.XIX.061} 
  } 
      `;

      var bibtex_diffusion_ccsp = `
  @inproceedings{yang2023diffusion,
    title={{Compositional Diffusion-Based Continuous Constraint Solvers}},
    author={Yang, Zhutian and Mao, Jiayuan and Du, Yilun and Wu, Jiajun and Tenenbaum, Joshua B. and Lozano-P{\\'e}rez, Tom{\\'a}s and Kaelbling, Leslie Pack},
    booktitle={Conference on Robot Learning},
    year={2023},
  }
      `;

      var bibtex_vlm_tamp = `
  @misc{yang2024guidinglonghorizontaskmotion,
      title={Guiding Long-Horizon Task and Motion Planning with Vision Language Models}, 
      author={Zhutian Yang and Caelan Garrett and Dieter Fox and Tomás Lozano-Pérez and Leslie Pack Kaelbling},
      year={2024},
      eprint={2410.02193},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2410.02193}, 
}
      `;

      var bibtex_popi = `
  @misc{ravan2024combiningplanningdiffusionmobility,
      title={Combining Planning and Diffusion for Mobility with Unknown Dynamics}, 
      author={Yajvan Ravan and Zhutian Yang and Tao Chen and Tomás Lozano-Pérez and Leslie Pack Kaelbling},
      year={2024},
      eprint={2410.06911},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2410.06911}, 
}
      `;
      
      
      // copy predefinedString once clicked
      function copyPiginetToClipboard() {    
          copyToClipboard(bibtex_piginet)
      }
      function copyDiffusionCCSPToClipboard() {  
          copyToClipboard(bibtex_diffusion_ccsp)
      }
      function copyVLMTAMPToClipboard() {    
          copyToClipboard(bibtex_vlm_tamp)
      }
      function copyPopiToClipboard() {    
          copyToClipboard(bibtex_popi)
      }

      function copyedNotice() { 
        // Show the notice and hide it after 2 seconds
        const notice = document.getElementById("copynotice");
        notice.classList.remove("hidden");
        notice.style.opacity = "1";
        setTimeout(() => {
            notice.style.opacity = "0";
            setTimeout(() => {
                notice.classList.add("hidden");
            }, 300);  // Wait for opacity transition to complete before hiding
        }, 2000);
      }

      // copy predefinedString once clicked
      function copyToClipboard(predefinedString) {    
          // Create a temporary textarea to utilize the execCommand('copy') method
          const tempTextArea = document.createElement("textarea");
          document.body.appendChild(tempTextArea);
          tempTextArea.value = predefinedString;
          tempTextArea.select();
          document.execCommand("copy");
          document.body.removeChild(tempTextArea);

          // alert("Text copied to clipboard: " + predefinedString);
          copyedNotice();
      }

    </script>

  </head>
  <body>

    <table>
      <tbody>
        <tr>
          <td class="bio">
            <h1 class="name center">Zhutian (Skye) Yang</h1>
            <div class="email center">
              <span><i style="color: grey">/ ju tin-yen young /</i></span>
              <span class="email-separator">&nbsp;&nbsp;&nbsp;&nbsp;</span>
              <span>ztyang {at} mit {dot} edu</span>
            </div>
            <div class="logos center">
              <a href="mailto:ztyang@mit.edu" target="_blank">
                <img class="logo" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/icon/email-icon.png?raw=true">
              </a>
              <a href="https://twitter.com/ZhutianYang_" target="_blank">
                <img class="logo" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/icon/twitter-icon.png?raw=true">
              </a>
              <a href="https://www.linkedin.com/in/zhutian-yang/" target="_blank">
                <img class="logo" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/icon/linkedin-icon.png?raw=true">
              </a>
              <a href="https://github.com/zt-yang" target="_blank">
                <img class="logo" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/icon/github-icon.png?raw=true">
              </a>
              <a href="https://cap.csail.mit.edu/engage/spotlights/zhutian-yang?utm_source=linkedin&amp;utm_medium=social&amp;utm_campaign=zhutianspot24" target="_blank">
                <img class="logo" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/icon/youtube-icon.png?raw=true">
              </a>
              <a href="https://scholar.google.com/citations?user=vW5LLmUAAAAJ&amp;hl=en" target="_blank">
                <img class="logo" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/icon/scholar-icon.png?raw=true">
              </a>
              <a href="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/ztyang-CV-2025.pdf?raw=true" target="_blank">
                <img class="logo" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/icon/cv-icon.png?raw=true" alt="">
              </a>
            </div>


            <!-- Images that appear above the table on phones -->
            <div class="mobile-photos">
              <img src="https://raw.githubusercontent.com/zt-yang/zhutian-yang-website/refs/heads/main/img/zhutianyang.jpg" alt="Zhutian Yang" width="100%">
            </div>

            <p>Hello! My name is Zhutian and I'm a final-year PhD candidate in robotics at MIT. I develop algorithms for solving long-horizon manipulation problems in geometrically complex environments. I use a combination of deep learning and planning methods. </p><p>I'm co-advised by Leslie Pack Kaelbling and Tomás Lozano-Pérez in the <a href="https://lis.csail.mit.edu/">Learning and Intelligent Systems group</a>. I'm currently a part-time research intern at <a href="https://www.tri.global/our-work/robotics">Toyota Research Institute</a> (TRI) Large Behavior Models team. I was an ex-<a href="https://research.nvidia.com/labs/srl/">NVIDIA</a> intern in the Seattle Robotics Lab. I obtained bachelor's degree in information engineering and media at NTU, Singapore.</p>
          </td>

          <td class="photo-column">
            <div class="photo">
              <img src="https://raw.githubusercontent.com/zt-yang/zhutian-yang-website/refs/heads/main/img/zhutianyang.jpg" alt="Zhutian Yang" width="100%">
              <!-- <img src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/aha.jpg?raw=true" alt="Aha" class="hover-img" width="100%"> -->
            </div>
          </td>
        </tr>

      </tbody>
    </table>

    <!-- <p style="margin-top: 0pt;"><b><i style="color: #e74c3c">Open to Work:</i></b> I'm looking for a Research Scientist position in industry robotic labs to continue working on mobile manipulation policy learning and vision language action models. I plan to defend in May/June 2025.</p> -->

    <div class="news-block">
      <p style="margin-top: 0pt; margin-bottom: -8pt;">
        <b><i style="color: #e74c3c">News:</i></b> 
        <ul class="parent-list">
          <li> [01/27/25]: VLM-TAMP was accepted to ICRA 2025. See you 19–23 May in Atlanta, Georgia! </li>
        </ul>
      </p>
    </div>

    <h2>Research</h2>
    Robots exhibiting long-horizon behavior, such as unpacking grocery bags and heating up the takeouts, must be able to <b>plan quickly and execute robustly</b> in <b>semantically rich, geometrically complex environments</b>. Enabling robots to perform language-instructed, multi-step mobile manipulation tasks requires <b>foundational models for planning and acting</b>. Training and evaluating such models in simulation for diverse robot embodiments holds significant economic and scientific potential. Towards building fully autonomous and intelligent robot systems, I've worked on the following problems in the field of robot learning and planning:
    <ul class="parent-list">
      <li> <b>Developing generalizable long-horizon visuomotor policies.</b>
        <!--  by conbining an <b>X-conditioned local policy</b> (where x can be language, goal bounding box, end-effector or base poses, hidden states, etc) and a high-level <b>X-generation planning model</b> (e.g., large fine-tuned VLM) -->
        <ul class="child-list">
          <li> 
            <span class="project at-tri">
              <img src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/icon/tri-logo.png?raw=true">Ongoing at TRI</span> 
            Developed a hierarchical multi-task policy architecture where a Robot Visual Planning Network generates language goals and guides a low-level language-conditioned multi-skill policy. 
          </li>
          <li> 
            <span class="project at-mit">
              <img src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/icon/csail-logo.png?raw=true">Ongoing at MIT</span> 
            Developing humanoid manipulation policies for multi-contact whole-body manipulation tasks by conbining diverse data sources collected in virtual and real environments.
          </li>
          <li> 
            <a class="project at-mit" href="https://yravan.github.io/plannerorderedpolicy" target="_blank">
              <img src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/icon/csail-logo.png?raw=true">PoPi</a> 
            Chain imitation learned policies conditioned on waypoints generated by motion planning for solving multi-step mobile manipulation of objects with unknown dynamics. 
          </li>
        </ul>
      </li>
      <li> <b>Generating long-horizon manipulation trajectories by combining learning-based methods and planning techniques.</b>
        <ul class="child-list">  
          <li> 
            <a class="project at-nvidia" href="https://zt-yang.github.io/vlm-tamp-robot" target="_blank"> 
              <img src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/icon/nvidia-logo.png?raw=true">VLM-TAMP</a> 
            Solve long-horizon manipulation problems for any robot embodiment by combining the geometric reasoning ability of TAMP and common-sense provided by pre-trained VLMs. </li>
          <li> 
            <a class="project at-mit" href="https://diffusion-ccsp.github.io" target="_blank">
              <img src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/icon/csail-logo.png?raw=true">Diffusion-CCSP</a> 
            Learn to plan for motion while satisfacing geometric collision-free, physical stability, and data-defined spatial constraints (e.g. packing non-convex objects in boxes). </li>
          <li> 
            <a class="project at-nvidia" href="https://piginet.github.io" target="_blank">
              <img src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/icon/nvidia-logo.png?raw=true">PiGi</a> 
            Learn to predict task plan feasibility from images of environments with articulated and movable obstacles using a transformer trained on a wide variety of procedurally generated scenes. </li>
          <!-- <li> Hierarchical planning and replanning in partially observable Task and Motion Planning problems. </li> -->
        </ul>
      </li>
    </ul>

    <!--div class="video-container">
      <iframe frameborder="0"
        src="https://www.youtube.com/embed/0WjD5FeXQB0">
      </iframe>
      <p class="caption"> Music Video: Robots learning to plan motion faster for household tasks. It's for a project I finished in 2022 at NVIDIA Seattle Robotics Lab, <a href="https://piginet.github.io/">https://piginet.github.io/</a> </p>
    </div-->

    <!-- <img class="figure" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/piginet_tasks.gif?raw=true">
    <p class="caption"> Figure: Robots learning to plan motion faster for rearrangement tasks </p> -->

    <h2>Publications</h2>
    <div id="copynotice" class="hidden">Bibtex copied to clipboard!</div>

    <ul class="publications">
      
      <div class="row">
        <div class="columnimg">
          <img class="figure default-img" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/research/icra25vlmtamp.gif?raw=true">
          <img class="figure hover-img" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/research/icra25method.png?raw=true">
        </div>
        <div class="columntext">

            <li class="pub">
              <p>
                <a href="https://zt-yang.github.io/vlm-tamp-robot" target="_blank" class="highlightable pub-title">Guiding Long-Horizon Task and Motion Planning with Vision Language Models</a>
              </p>
              <p><span class="self-author">Zhutian Yang</span>, Caelan Reed Garrett, Leslie Pack Kaelbling, Tomás Lozano-Pérez, and Dieter Fox</p>
              <p class="conference"><i>ICRA 2025; CoRL 2024 LangRob Workshop (Spotlight)</i></p>
              <p><p style="color:#e67e22"><i><b>TLDR</b>: Pretrained VLMs make mistakes in predicting robot actions when prompted with open language goals, so we use VLMs to break down long-horizon goals into subgoals, which are then solved by TAMP, in an interative replanning system. It's used to solve problems that involve interactions with 20+ objects and require 30-50 actions to complete.</i></p><a class="highlightable" href="http://arxiv.org/abs/2410.02193" target="_blank">Paper</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://zt-yang.github.io/vlm-tamp-robot" target="_blank">Project Page</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://github.com/Learning-and-Intelligent-Systems/kitchen-worlds/tree/main" target="_blank">Code</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable buttona" onclick="copyVLMTAMPToClipboard();">Bibtex</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://youtu.be/1JDua3opFuM" target="_blank">Talk</a>&nbsp;&nbsp;|&nbsp;&nbsp;<span id="showvlmtamp" class="highlightable posterbutton"  onclick="document.getElementById('vlmtamp').style.display='block'; document.getElementById('hidevlmtamp').style.display='inline'; document.getElementById('showvlmtamp').style.display='none'" onmouseout="normalSpan(this)">Poster (click to show)</span>
    <span id="hidevlmtamp" class="highlightable posterbutton" style="display:none" onclick="document.getElementById('vlmtamp').style.display='none'; document.getElementById('hidevlmtamp').style.display='none'; document.getElementById('showvlmtamp').style.display='inline'"   onmouseout="normalSpan(this)"">Poster (click to hide)</span>
    <iframe class="posterlink" id="vlmtamp" src="https://drive.google.com/file/d/1mawmZZYIiHII_5xag9E3Va554wRQ9onB/preview" style="display:none;" width="800px"
    height="991px" allow="autoplay"></iframe></p>
            </li>

        </div>
      </div>

    
      <div class="row">
        <div class="columnimg">
          <img class="figure default-img" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/research/icra25popi.gif?raw=true">
          <img class="figure hover-img" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/research/icra25popimethod.png?raw=true">
        </div>
        <div class="columntext">

            <li class="pub">
              <p>
                <a href="https://yravan.github.io/plannerorderedpolicy" target="_blank" class="highlightable pub-title">Combining Planning and Diffusion for Mobility with Unknown Dynamics</a>
              </p>
              <p>Yajvan Ravan, <span class="self-author">Zhutian Yang</span>, Tao Chen, Leslie Pack Kaelbling, and Tomás Lozano-Pérez</p>
              <p class="conference"><i>In Submission</i></p>
              <p><p style="color:#e67e22"><i><b>TLDR</b>: Rearranging large objects with unprediatble dynamics is hard because the relative pose between robot and object is changing. Diffusion policies that output global robot configurations struggle to generalize to new initial and goal conditions, or new environments and objects. So, we use motion planning to generating waypoints that guide a local diffusion policy, which is trained to achieve relative movements of the chair.</i></p><a class="highlightable" href="https://arxiv.org/abs/2410.06911" target="_blank">Paper</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://yravan.github.io/plannerorderedpolicy" target="_blank">Project Page</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable buttona" onclick="copyPopiToClipboard();">Bibtex</a></p>
            </li>

        </div>
      </div>

    
      <div class="row">
        <div class="columnimg">
          <img class="figure default-img" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/research/corl23packing.gif?raw=true">
          <img class="figure hover-img" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/research/corl23method.gif?raw=true">
        </div>
        <div class="columntext">

            <li class="pub">
              <p>
                <a href="https://diffusion-ccsp.github.io/" target="_blank" class="highlightable pub-title">Compositional Diffusion-Based Continuous Constraint Solvers</a>
              </p>
              <p><span class="self-author">Zhutian Yang</span>, Jiayuan Mao, Yilun Du, Jiajun Wu, Joshua Brett Tenenbaum, Tomás Lozano-Pérez, and Leslie Pack Kaelbling</p>
              <p class="conference"><i>CoRL 2023</i></p>
              <p><p style="color:#e67e22"><i><b>TLDR</b>: Multi-step manipulation problems involve a lot of collision-free, physical stability, and culture-defined spatial constraints. Conventional methods usually solve it by sampling then rejection, which is too slow. Therefore, we find global solutions by diffusion-based optimization, using diffusion models trained for each contraint type.</i></p><a class="highlightable" href="http://arxiv.org/abs/2309.00966" target="_blank">Paper</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://diffusion-ccsp.github.io/" target="_blank">Project Page</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://github.com/zt-yang/diffusion-ccsp" target="_blank">Code</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable buttona" onclick="copyDiffusionCCSPToClipboard();">Bibtex</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://youtu.be/mhtYU7IevtE" target="_blank">Talk</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://news.mit.edu/2023/new-technique-helps-robots-pack-objects-tight-space-1017" target="_blank">MIT News</a>&nbsp;&nbsp;|&nbsp;&nbsp;<span id="showdiffusion-ccsp" class="highlightable posterbutton"  onclick="document.getElementById('diffusion-ccsp').style.display='block'; document.getElementById('hidediffusion-ccsp').style.display='inline'; document.getElementById('showdiffusion-ccsp').style.display='none'" onmouseout="normalSpan(this)">Poster (click to show)</span>
    <span id="hidediffusion-ccsp" class="highlightable posterbutton" style="display:none" onclick="document.getElementById('diffusion-ccsp').style.display='none'; document.getElementById('hidediffusion-ccsp').style.display='none'; document.getElementById('showdiffusion-ccsp').style.display='inline'"   onmouseout="normalSpan(this)"">Poster (click to hide)</span>
    <iframe class="posterlink" id="diffusion-ccsp" src="https://drive.google.com/file/d/1uKmcsuBO0D5o5QrXBCKQfx4cPTTJ8mGv/preview" style="display:none;" width="800px"
    height="600px" allow="autoplay"></iframe></p>
            </li>

        </div>
      </div>

    
      <div class="row">
        <div class="columnimg">
          <img class="figure default-img" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/research/rss23kitchens.gif?raw=true">
          <img class="figure hover-img" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/research/rss23method.png?raw=true">
        </div>
        <div class="columntext">

            <li class="pub">
              <p>
                <a href="https://piginet.github.io/" target="_blank" class="highlightable pub-title">Sequence-Based Plan Feasibility Prediction for Efficient Task and Motion Planning</a>
              </p>
              <p><span class="self-author">Zhutian Yang</span>, Caelan Reed Garrett, Leslie Pack Kaelbling, Tomás Lozano-Pérez, and Dieter Fox</p>
              <p class="conference"><i>RSS 2023</i></p>
              <p><p style="color:#e67e22"><i><b>TLDR</b>: In long-horizon mobile manipulation problems in complex environments with lots of articulated and movable obstacles, task and motion planners spend most computation on solving motion planning problems that aren't solvable. So we train a plan feasibility prediction model that quickly sort candidate plans by their likelihood of success using visual and language features of the problem, which cuts down planning time by 50 - 80 %.</i></p><p style="color: grey"><i>&#128293; We won Best Paper Runner-Up in CoRL 2022 Workshop on Learning, Perception, and Abstraction for Long-Horizon Planning</i></p><a class="highlightable" href="https://arxiv.org/abs/2211.01576" target="_blank">Paper</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://piginet.github.io/" target="_blank">Project Page</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://github.com/Learning-and-Intelligent-Systems/kitchen-worlds/tree/main" target="_blank">Code</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable buttona" onclick="copyPiginetToClipboard();">Bibtex</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://youtu.be/QXmcu9fVnak?t=23991" target="_blank">Talk</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://news.mit.edu/2023/ai-helps-household-robots-cut-planning-time-half-0714" target="_blank">MIT News</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://techcrunch.com/2023/07/07/mit-develops-a-motion-and-task-planning-system-for-home-robots/" target="_blank">Tech Crunch</a>&nbsp;&nbsp;|&nbsp;&nbsp;<span id="showpiginet" class="highlightable posterbutton"  onclick="document.getElementById('piginet').style.display='block'; document.getElementById('hidepiginet').style.display='inline'; document.getElementById('showpiginet').style.display='none'" onmouseout="normalSpan(this)">Poster (click to show)</span>
    <span id="hidepiginet" class="highlightable posterbutton" style="display:none" onclick="document.getElementById('piginet').style.display='none'; document.getElementById('hidepiginet').style.display='none'; document.getElementById('showpiginet').style.display='inline'"   onmouseout="normalSpan(this)"">Poster (click to hide)</span>
    <iframe class="posterlink" id="piginet" src="https://drive.google.com/file/d/1cpzkM4o91fNaOBnwIM_-YC58Yt5vOHML/preview" style="display:none;" width="800px"
    height="440px" allow="autoplay"></iframe></p>
            </li>

        </div>
      </div>

    
      <div class="row">
        <div class="columnimg">
          <img class="figure default-img" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/research/iclr22cover_cropped.jpg?raw=true">
          <img class="figure hover-img" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/research/iclr22cover_cropped.jpg?raw=true">
        </div>
        <div class="columntext">

            <li class="pub">
              <p>
                <a href="" target="_blank" class="highlightable pub-title">Let’s Handle It: Generalizable Manipulation of Articulated Objects</a>
              </p>
              <p><span class="self-author">Zhutian Yang</span>, and Aidan Curtis</p>
              <p class="conference"><i>ICRL 2022 Workshop on Generalizable Policy Learning in the Physical World (Spotlight)</i></p>
              <p><p style="color: grey"><i>&#128293; We won 2nd place in the ManiSkill Challenge 2022 Robotics Track</i></p><a class="highlightable" href="https://openreview.net/pdf?id=SObVnEp4yb9" target="_blank">Paper</a>&nbsp;&nbsp;|&nbsp;&nbsp;<span id="showmaniskill" class="highlightable posterbutton"  onclick="document.getElementById('maniskill').style.display='block'; document.getElementById('hidemaniskill').style.display='inline'; document.getElementById('showmaniskill').style.display='none'" onmouseout="normalSpan(this)">Poster (click to show)</span>
    <span id="hidemaniskill" class="highlightable posterbutton" style="display:none" onclick="document.getElementById('maniskill').style.display='none'; document.getElementById('hidemaniskill').style.display='none'; document.getElementById('showmaniskill').style.display='inline'"   onmouseout="normalSpan(this)"">Poster (click to hide)</span>
    <iframe class="posterlink" id="maniskill" src="https://drive.google.com/file/d/1arsOQ_e9Ydt12QbYNd6uIswdKFz-fcHP/preview" style="display:none;" width="800px"
    height="534px" allow="autoplay"></iframe></p>
            </li>

        </div>
      </div>

    
      <div class="row">
        <div class="columnimg">
          <img class="figure default-img" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/research/robochef19cover.png?raw=true">
          <img class="figure hover-img" src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/research/robochef19cover.png?raw=true">
        </div>
        <div class="columntext">

            <li class="pub">
              <p>
                <a href="" target="_blank" class="highlightable pub-title">Flexibly Instructable Robots</a>
              </p>
              <p><span class="self-author">Zhutian Yang</span>, Patrick Henry Winston, and David Hsu</p>
              <p class="conference"><i>Undergraduate thesis work; Also appeared in Advances in Cognitive Systems 2019 and DSpace@MIT</i></p>
              <p><a class="highlightable" href="https://dspace.mit.edu/handle/1721.1/119668" target="_blank">Paper</a>&nbsp;&nbsp;|&nbsp;&nbsp;<a class="highlightable" href="https://youtu.be/oGgHGs0lKPU" target="_blank">Video Demo</a>&nbsp;&nbsp;|&nbsp;&nbsp;<span id="showrobotchef" class="highlightable posterbutton"  onclick="document.getElementById('robotchef').style.display='block'; document.getElementById('hiderobotchef').style.display='inline'; document.getElementById('showrobotchef').style.display='none'" onmouseout="normalSpan(this)">Poster (click to show)</span>
    <span id="hiderobotchef" class="highlightable posterbutton" style="display:none" onclick="document.getElementById('robotchef').style.display='none'; document.getElementById('hiderobotchef').style.display='none'; document.getElementById('showrobotchef').style.display='inline'"   onmouseout="normalSpan(this)"">Poster (click to hide)</span>
    <iframe class="posterlink" id="robotchef" src="https://drive.google.com/file/d/1FMIQsfJN5r8Rkqw435AHijorCHgjXDkq/preview" style="display:none;" width="800px"
    height="534px" allow="autoplay"></iframe></p>
            </li>

        </div>
      </div>

    
    </ul>

    <h2>Services</h2>

    <ul>
      <li>Reviewed for 
        <a href="https://en.wikipedia.org/wiki/International_Conference_on_Learning_Representations" target="_blank">ICLR</a>, 
        <a href="https://ieee-iros.org/" target="_blank">IROS</a>, 
        <a href="https://www.ieee-ras.org/publications/ra-l" target="_blank">RA-L</a>, 
        <a href="https://aaai.org/aaai-conference/" target="_blank">AAAI</a>, 
        <a href="https://2024.ieee-icra.org/" target="_blank">ICRA</a>, and
        <a href="https://roboticsconference.org/" target="_blank">RSS</a>.
      </li>
      <li>Co-organized <a href="https://vlmnm-workshop.github.io/" target="_blank">ICRA 2024 Workshop on Vision-Language Models for Navigation and Manipulation (VLMNM)</a>.</li>
      <li>Co-organized <a href="https://zt-yang.github.io/rss23-l4tamp-workshop/" target="_blank">RSS 2023 Workshop on Learning for Task and Motion Planning (LTAMP)</a>.</li>
      <div style="margin-top: 15px; margin-bottom: 10px;">
        <a href="https://zt-yang.github.io/rss23-l4tamp-workshop/">
          <img src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/service/RSS23LTAMP.png?raw=true" style="width: 49%"/>
        </a>
        <a href="https://vlmnm-workshop.github.io/">
          <img src="https://github.com/zt-yang/zhutian-yang-website/blob/main/img/service/ICRA24VLMNM.png?raw=true" style="width: 49%"/>
        </a>
      </div>
      <li>Serving as a Student Counselor in <a href="http://eecsrefs.mit.edu/">EECS Resources for Easing Friction and Stress</a> since 2020 and helped 10+ graduate students through stressful situations such as changing advisors and family conflicts.</li>
      <li>Served as Teaching Assistant to MIT 6.036 Introduction to Machine Learning in Spring 2022.</li>
    </ul>

    <h2>Misc.</h2>

    <ul>
      <li>I'm an AFAA certified group exercise instructor, specializing in kickboxing.</li>
      <li>If thrown out of robotics research and threatened to never code again, I would do improv musical comedy.</li>
      <li>Wait, I could actually fight back.</li>
    </ul>

  </body>
</html>
